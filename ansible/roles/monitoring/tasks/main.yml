---
- name: Create dirs for monitoring stack
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: "0755"
  loop:
    - /opt/monitoring
    - /opt/monitoring/prometheus
    - /opt/monitoring/prometheus/rules 
    - /opt/monitoring/grafana/provisioning/datasources
    - /opt/monitoring/grafana/provisioning/dashboards
    - /opt/monitoring/grafana/dashboards
    - /opt/monitoring/loki
    - /opt/monitoring/promtail
    - /opt/monitoring/alertmanager

- name: Ensure Loki subdirs exist
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: "0755"
  loop:
    - /opt/monitoring/loki/chunks
    - /opt/monitoring/loki/index
    - /opt/monitoring/loki/boltdb-cache
    - /opt/monitoring/loki/compactor
    - /opt/monitoring/loki/rules

- name: Create alertmanager dir
  file:
    path: /opt/monitoring/alertmanager
    state: directory
    owner: root
    group: root
    mode: "0755"

- name: Drop Prometheus config
  copy:
    dest: /opt/monitoring/prometheus/prometheus.yml
    mode: "0644"
    content: |
      global:
        scrape_interval: 15s
        evaluation_interval: 15s

      alerting:
        alertmanagers:
          - static_configs:
              - targets: ['alertmanager:9093']

      rule_files:
        - /etc/prometheus/rules/*.yml
        - /etc/prometheus/alert_rules.yml

      scrape_configs:
        # node_exporter
        - job_name: 'node'
          static_configs:
            - targets: ['node-exporter:9100']

        # cAdvisor
        - job_name: 'cadvisor'
          static_configs:
            - targets: ['cadvisor:8080']

        # blackbox
        - job_name: 'blackbox'
          metrics_path: /probe
          params:
            module: [http_2xx]
          static_configs:
            - targets:
                - "https://{{ app_domain }}"
          relabel_configs:
            - source_labels: [__address__]
              target_label: __param_target
            - source_labels: [__param_target]
              target_label: instance
            - target_label: __address__
              replacement: blackbox-exporter:9115

- name: Drop Prometheus alert rules
  copy:
    dest: /opt/monitoring/prometheus/alert_rules.yml
    mode: "0644"
    content: |
      groups:
        - name: uptime_http
          rules:
            - alert: SiteDown
              expr: probe_success{instance="https://{{ app_domain }}"} == 0
              for: 1m
              labels: { severity: critical }
              annotations:
                summary: "Site {{ app_domain }} is down"
                description: "Blackbox probe failed for {{ app_domain }}"

            - alert: SlowHTTP
              expr: probe_duration_seconds{instance="https://{{ app_domain }}"} > 0.5
              for: 5m
              labels: { severity: warning }
              annotations:
                summary: "Slow HTTP for {{ app_domain }}"
                description: "p95 probe_duration_seconds > 0.5s"

        - name: host_health
          rules:
            - alert: HighCPU
              expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0.7
              for: 10m
              labels: { severity: warning }
              annotations:
                summary: "High CPU on node"
                description: "5m avg CPU > 70%"

            - alert: LowMemory
              expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.1
              for: 10m
              labels: { severity: warning }
              annotations:
                summary: "Low memory"
                description: "Available RAM < 10%"

            - alert: LowDiskRoot
              expr: node_filesystem_avail_bytes{mountpoint="/",fstype!~"tmpfs|overlay"} < 5e+9
              for: 10m
              labels: { severity: warning }
              annotations:
                summary: "Low disk space on /"
                description: "Free space on / < 5 GB"

- name: Drop blackbox config
  copy:
    dest: /opt/monitoring/blackbox.yml
    mode: "0644"
    content: |
      modules:
        http_2xx:
          prober: http
          timeout: 10s
          http:
            method: GET
            preferred_ip_protocol: "ip4"
            valid_status_codes: []  # 2xx by default
            tls_config:
              insecure_skip_verify: false

- name: Drop Loki config (single node, filesystem)
  copy:
    dest: /opt/monitoring/loki/config.yml
    mode: "0644"
    content: |
      auth_enabled: false

      server:
        http_listen_port: 3100

      # Односерверный ring
      ingester:
        lifecycler:
          ring:
            kvstore:
              store: inmemory
            replication_factor: 1
        chunk_idle_period: 3m
        chunk_retain_period: 1m

      # ВАЖНО: compactor должен знать свой рабочий каталог и стор
      compactor:
        working_directory: /loki/compactor
        shared_store: filesystem
        compaction_interval: 5m

      schema_config:
        configs:
          - from: 2024-01-01
            store: boltdb-shipper
            object_store: filesystem
            schema: v13
            index:
              prefix: index_
              period: 24h

      storage_config:
        boltdb_shipper:
          active_index_directory: /loki/index
          cache_location: /loki/boltdb-cache
          shared_store: filesystem
        filesystem:
          directory: /loki/chunks

      limits_config:
        ingestion_rate_mb: 8
        ingestion_burst_size_mb: 16
        reject_old_samples: true
        reject_old_samples_max_age: 168h

      chunk_store_config:
        max_look_back_period: 0s

      table_manager:
        retention_deletes_enabled: true
        retention_period: 168h

      ruler:
        storage:
          type: local
          local:
            directory: /loki/rules
        rule_path: /tmp/loki/rules
        ring:
          kvstore:
            store: inmemory

- name: Drop Promtail config (nginx + docker)
  copy:
    dest: /opt/monitoring/promtail/promtail-config.yml
    mode: "0644"
    content: |
      server:
        http_listen_port: 9080
        grpc_listen_port: 0

      positions:
        filename: /var/lib/promtail/positions.yaml

      clients:
        - url: http://loki:3100/loki/api/v1/push

      scrape_configs:
        # Nginx access/error
        - job_name: nginx-logs
          static_configs:
            - targets: [localhost]
              labels:
                job: nginx
                __path__: /var/log/nginx/*.log

        # Docker контейнеры (json)
        - job_name: docker-containers
          pipeline_stages:
            - docker: {}
          static_configs:
            - targets: [localhost]
              labels:
                job: docker
                __path__: /var/lib/docker/containers/*/*-json.log

- name: Drop Grafana datasource for Prometheus (fixed UID)
  copy:
    dest: /opt/monitoring/grafana/provisioning/datasources/prometheus.yml
    mode: "0644"
    content: |
      apiVersion: 1
      datasources:
        - name: Prometheus
          uid: prometheus
          type: prometheus
          access: proxy
          url: http://prometheus:9090
          isDefault: true

- name: Drop Grafana datasource for Loki (fixed UID)
  copy:
    dest: /opt/monitoring/grafana/provisioning/datasources/loki.yml
    mode: "0644"
    content: |
      apiVersion: 1
      datasources:
        - name: Loki
          uid: loki
          type: loki
          access: proxy
          url: http://loki:3100
          isDefault: false

- name: Provision Grafana dashboard provider
  copy:
    dest: /opt/monitoring/grafana/provisioning/dashboards/provider.yml
    mode: "0644"
    content: |
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          options:
            path: /var/lib/grafana/dashboards
            foldersFromFilesStructure: true

- name: Download Grafana dashboard - Node Exporter (ID 1860)
  get_url:
    url: https://grafana.com/api/dashboards/1860/revisions/latest/download
    dest: /opt/monitoring/grafana/dashboards/node-exporter.json
    mode: "0644"

- name: Drop Grafana dashboard (Blackbox — mini, fixed types)
  copy:
    dest: /opt/monitoring/grafana/dashboards/blackbox-local.json
    mode: "0644"
    content: |-
      {% raw %}
      {
        "title": "Blackbox — {{ app_domain }}",
        "timezone": "browser",
        "schemaVersion": 39,
        "version": 1,
        "panels": [
          {
            "type": "stat",
            "title": "Status",
            "gridPos": { "h": 6, "w": 8, "x": 0, "y": 0 },
            "targets": [
              { "expr": "probe_success{job=\"blackbox\", instance=\"$target\"}", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          },
          {
            "type": "stat",
            "title": "HTTP Code",
            "gridPos": { "h": 6, "w": 8, "x": 8, "y": 0 },
            "targets": [
              { "expr": "probe_http_status_code{job=\"blackbox\", instance=\"$target\"}", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          },
          {
            "type": "timeseries",
            "title": "Probe duration (s)",
            "gridPos": { "h": 8, "w": 16, "x": 0, "y": 6 },
            "targets": [
              { "expr": "avg(probe_duration_seconds{job=\"blackbox\", instance=\"$target\"})", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          },
          {
            "type": "stat",
            "title": "SSL expiry (days)",
            "gridPos": { "h": 6, "w": 16, "x": 0, "y": 14 },
            "targets": [
              { "expr": "(probe_ssl_earliest_cert_expiry{job=\"blackbox\", instance=\"$target\"} - time()) / 86400", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          }
        ],
        "templating": {
          "list": [
            {
              "name": "target",
              "type": "query",
              "label": "target",
              "datasource": {"type":"prometheus","uid":"prometheus"},
              "query": "label_values(probe_success{job=\"blackbox\"}, instance)",
              "refresh": 2,
              "current": { "text": "https://{{ app_domain }}", "value": "https://{{ app_domain }}" }
            }
          ]
        }
      }
      {% endraw %}

- name: Drop Grafana dashboard (Docker / cAdvisor — mini)
  copy:
    dest: /opt/monitoring/grafana/dashboards/cadvisor-local.json
    mode: "0644"
    content: |-
      {% raw %}
      {
        "title": "Docker containers (cAdvisor)",
        "schemaVersion": 39,
        "version": 1,
        "panels": [
          {
            "type": "timeseries",
            "title": "CPU (cores) — rate",
            "gridPos": { "h": 8, "w": 24, "x": 0, "y": 0 },
            "targets": [
              { "expr": "sum by (name) (rate(container_cpu_usage_seconds_total{job=\"cadvisor\", name=~\"$container\"}[5m]))", "legendFormat": "{{ name }}", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          },
          {
            "type": "timeseries",
            "title": "Memory usage (bytes)",
            "gridPos": { "h": 8, "w": 24, "x": 0, "y": 8 },
            "targets": [
              { "expr": "container_memory_usage_bytes{job=\"cadvisor\", name=~\"$container\"}", "legendFormat": "{{ name }}", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          },
          {
            "type": "timeseries",
            "title": "Network Rx / Tx (bytes/s)",
            "gridPos": { "h": 8, "w": 24, "x": 0, "y": 16 },
            "targets": [
              { "expr": "sum by (name) (rate(container_network_receive_bytes_total{job=\"cadvisor\", name=~\"$container\"}[5m]))", "legendFormat": "{{ name }} rx", "datasource": {"type":"prometheus","uid":"prometheus"} },
              { "expr": "sum by (name) (rate(container_network_transmit_bytes_total{job=\"cadvisor\", name=~\"$container\"}[5m]))", "legendFormat": "{{ name }} tx", "datasource": {"type":"prometheus","uid":"prometheus"} }
            ]
          }
        ],
        "templating": {
          "list": [
            {
              "name": "container",
              "type": "query",
              "label": "container",
              "datasource": {"type":"prometheus","uid":"prometheus"},
              "query": "label_values(container_memory_usage_bytes{job=\"cadvisor\"}, name)",
              "refresh": 2,
              "multi": true,
              "includeAll": true,
              "current": { "text": "All", "value": "$__all" }
            }
          ]
        }
      }
      {% endraw %}

- name: Drop Grafana dashboard (Node mini)
  copy:
    dest: /opt/monitoring/grafana/dashboards/node-mini.json
    mode: "0644"
    content: |-
      {% raw %}
      {
        "title": "Node Exporter — mini",
        "schemaVersion": 39,
        "version": 1,
        "templating": {
          "list": [
            {
              "name": "instance",
              "type": "query",
              "label": "Instance",
              "datasource": {"type":"prometheus","uid":"prometheus"},
              "query": "label_values(node_uname_info, instance)",
              "refresh": 2,
              "current": { "text": "node-exporter:9100", "value": "node-exporter:9100" }
            }
          ]
        },
        "panels": [
          { "type": "stat", "title": "CPU Burst", "gridPos": {"h":6,"w":6,"x":0,"y":0},
            "targets":[{"expr":"100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\", instance=\"$instance\"}[5m])) * 100)","datasource":{"type":"prometheus","uid":"prometheus"}}]},
          { "type": "stat", "title": "Sys Load", "gridPos": {"h":6,"w":6,"x":6,"y":0},
            "targets":[{"expr":"node_load1{instance=\"$instance\"}","datasource":{"type":"prometheus","uid":"prometheus"}}]},
          { "type": "stat", "title": "RAM Used %", "gridPos": {"h":6,"w":6,"x":12,"y":0},
            "targets":[{"expr":"(1 - (node_memory_MemAvailable_bytes{instance=\"$instance\"} / node_memory_MemTotal_bytes{instance=\"$instance\"})) * 100","datasource":{"type":"prometheus","uid":"prometheus"}}]},
          { "type": "stat", "title": "Root FS Used %", "gridPos": {"h":6,"w":6,"x":18,"y":0},
            "targets":[{"expr":"(1 - (node_filesystem_avail_bytes{mountpoint=\"/\",fstype!~\"tmpfs|overlay\",instance=\"$instance\"} / node_filesystem_size_bytes{mountpoint=\"/\",fstype!~\"tmpfs|overlay\",instance=\"$instance\"})) * 100","datasource":{"type":"prometheus","uid":"prometheus"}}]},
          { "type": "timeseries", "title": "CPU Basic", "gridPos": {"h":8,"w":12,"x":0,"y":6},
            "targets":[{"expr":"1 - avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\",instance=\"$instance\"}[5m]))","legendFormat":"cpu used","datasource":{"type":"prometheus","uid":"prometheus"}}]},
          { "type": "timeseries", "title": "Memory Basic", "gridPos": {"h":8,"w":12,"x":12,"y":6},
            "targets":[
              {"expr":"node_memory_MemTotal_bytes{instance=\"$instance\"}","legendFormat":"Total","datasource":{"type":"prometheus","uid":"prometheus"}},
              {"expr":"node_memory_MemTotal_bytes{instance=\"$instance\"} - node_memory_MemAvailable_bytes{instance=\"$instance\"}","legendFormat":"Used","datasource":{"type":"prometheus","uid":"prometheus"}}
            ]},
          { "type": "timeseries", "title": "Disk Space Used Basic", "gridPos": {"h":8,"w":12,"x":0,"y":14},
            "targets":[{"expr":"(node_filesystem_size_bytes{mountpoint=\"/\",fstype!~\"tmpfs|overlay\",instance=\"$instance\"} - node_filesystem_avail_bytes{mountpoint=\"/\",fstype!~\"tmpfs|overlay\",instance=\"$instance\"})","legendFormat":"Used /","datasource":{"type":"prometheus","uid":"prometheus"}}]},
          { "type": "stat", "title": "Uptime", "gridPos": {"h":8,"w":12,"x":12,"y":14},
            "targets":[{"expr":"time() - node_boot_time_seconds{instance=\"$instance\"}","datasource":{"type":"prometheus","uid":"prometheus"}}]}
        ]
      }
      {% endraw %}

- name: Drop Alertmanager config (Telegram)
  copy:
    dest: /opt/monitoring/alertmanager/alertmanager.yml
    mode: "0644"
    content: |
      route:
        receiver: 'telegram'
        group_by: ['alertname']
        group_wait: 10s
        group_interval: 1m
        repeat_interval: 2h

      receivers:
        - name: 'telegram'
          telegram_configs:
            - bot_token: "{{ telegram_bot_token }}"
              chat_id: {{ telegram_chat_id }}
              api_url: "https://api.telegram.org"
              parse_mode: "HTML"
              message: |-
                {% raw %}
                <b>{{ .Status | toUpper }}</b> {{ .CommonLabels.alertname }}
                {{ range .Alerts }}
                • <b>{{ .Labels.severity }}</b> — {{ .Annotations.summary }}
                  {{ .Annotations.description }}
                {{ end }}
                {% endraw %}

- name: Drop monitoring docker-compose
  copy:
    dest: /opt/monitoring/docker-compose.yml
    mode: "0644"
    content: |
      services:
        prometheus:
          image: prom/prometheus:v2.55.0
          command:
            - --config.file=/etc/prometheus/prometheus.yml
            - --storage.tsdb.retention.time=15d
          volumes:
            - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
            - ./prometheus/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
            - prometheus-data:/prometheus
          ports: ["127.0.0.1:9090:9090"]
          depends_on: [node-exporter, blackbox-exporter, alertmanager]
          restart: unless-stopped

        alertmanager:
          image: prom/alertmanager:v0.27.0
          command:
            - --config.file=/etc/alertmanager/alertmanager.yml
          volumes:
            - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
          ports: ["127.0.0.1:9093:9093"]
          restart: unless-stopped

        node-exporter:
          image: prom/node-exporter:v1.8.2
          command:
            - '--path.procfs=/host/proc'
            - '--path.sysfs=/host/sys'
            - '--path.rootfs=/rootfs'
            - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|run|var/lib/docker/.+)($|/)'
          volumes:
            - /proc:/host/proc:ro
            - /sys:/host/sys:ro
            - /:/rootfs:ro
          restart: unless-stopped

        cadvisor:
          image: gcr.io/cadvisor/cadvisor:v0.49.1
          privileged: true
          volumes:
            - /:/rootfs:ro
            - /var/run:/var/run:ro
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
          ports:
            - "127.0.0.1:8080:8080"
          restart: unless-stopped

        blackbox-exporter:
          image: prom/blackbox-exporter:v0.25.0
          command:
            - --config.file=/etc/blackbox/blackbox.yml
          volumes:
            - ./blackbox.yml:/etc/blackbox/blackbox.yml:ro
          ports:
            - "127.0.0.1:9115:9115"
          restart: unless-stopped

        loki:
          image: grafana/loki:2.9.8
          command: -config.file=/etc/loki/config.yml
          user: "0:0"
          volumes:
            - ./loki/config.yml:/etc/loki/config.yml:ro
            - ./loki:/loki
          ports:
            - "127.0.0.1:3100:3100"
          restart: unless-stopped

        promtail:
          image: grafana/promtail:2.9.8
          command: -config.file=/etc/promtail/promtail-config.yml
          volumes:
            - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml:ro
            - /var/log:/var/log:ro
            - /var/lib/docker/containers:/var/lib/docker/containers:ro
            - /var/lib/promtail:/var/lib/promtail
          restart: unless-stopped

        grafana:
          image: grafana/grafana:11.2.0
          environment:
            GF_SECURITY_ADMIN_USER: admin
            GF_SECURITY_ADMIN_PASSWORD: "{{ grafana_admin_password | default('admin') }}"
            GF_PATHS_PROVISIONING: /etc/grafana/provisioning
          volumes:
            - ./grafana/provisioning:/etc/grafana/provisioning
            - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
            - grafana-data:/var/lib/grafana
          ports:
            - "127.0.0.1:3000:3000"
          depends_on:
            - prometheus
            - loki
          restart: unless-stopped

      volumes:
        grafana-data: {}
        prometheus-data: {}

- name: Up monitoring stack (force recreate everything)
  args:
    chdir: /opt/monitoring
  shell: |
    docker compose -f docker-compose.yml down --remove-orphans || true
    docker compose -f docker-compose.yml pull
    docker compose -f docker-compose.yml up -d \
      --force-recreate \
      --remove-orphans

- name: Wait Grafana is listening on 3000
  wait_for:
    host: 127.0.0.1
    port: 3000
    timeout: 90

- name: Check Grafana login with provided password
  uri:
    url: http://127.0.0.1:3000/api/login/ping
    method: GET
    force_basic_auth: true
    url_username: admin
    url_password: "{{ grafana_admin_password }}"
    status_code: [200, 401]
    return_content: false
  register: grafana_ping
  no_log: true

- name: Decide if password reset is needed
  set_fact:
    grafana_need_reset: "{{ grafana_ping.status != 200 }}"

- name: Reset Grafana admin password (only if needed)
  shell: |
    docker exec monitoring-grafana-1 grafana cli admin reset-admin-password '{{ grafana_admin_password }}'
  when: grafana_need_reset | bool
  register: grafana_pw
  changed_when: "'Admin password changed successfully' in grafana_pw.stdout"
  failed_when: >
    grafana_pw.rc != 0 and
    ('Admin password changed successfully' not in grafana_pw.stdout)
  no_log: true

- name: Mark deployed (marker file, но не блокирует апдейты)
  file:
    path: /opt/monitoring/.deployed
    state: touch
